{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Boat rentals (24 hour analysis)\n",
    "\n",
    "### Project Brief\n",
    "\n",
    "Congratulations, you have just landed your first job as a data scientist at ABC! ABC is a website that allows users to advertise their used boats for sale. To boost traffic to the website, the product manager wants to prevent listing boats that do not receive many views.\n",
    "\n",
    "The product manager wants to know if you can develop a model to predict the number of views a listing will receive based on the boat's features. She would consider using your model if, on average, the predictions were only 50% off of the true number of views a listing would receive.\n",
    "\n",
    "In addition, she has noticed that many users never complete the introductory survey to list their boat. She suspects that it is too long and has asked you whether some features are more predictive of views than others. If so, she may be able to trim the length of the survey and increase the number of people who sign up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Table of Content\n",
    "* [The Dataset](#dataset)\n",
    "* [Data cleaning](#clean)\n",
    "* [Feature Selection & Model Evaluation](#feature)\n",
    "* [Final recommendations](#recommend)\n",
    "* [Future work](#future)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Dataset <a class=\"anchor\" id=\"dataset\"/>\n",
    "\n",
    "Finding the perfect recipe to increase the sign-up ratio and the traffic into the website is definitely challenging. It gets even more so, if the sign-up experience is full of endless questions. This notebook was created to attempt facing these challenges. To do so, we have been provided with a dataset that consists 9888 entries. Even though the dataset does not seem that big, it's extremely inconsistent which we will see under [Data cleaning](#clean). Here is the overview of dataset and its columns:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dissagaliyeva/boat-rentals/master/data/data.png\" width=\"500\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data cleaning <a class=\"anchor\" id=\"clean\"/>\n",
    "\n",
    "The first step is to identify the problems and create the appropriate action plan. After careful analysis, the following patterns were revealed:\n",
    "- Location, Manufacturer, and Boat Type columns have a huge number of unique values. This will be a problem when building models. For example, Location column contains 2995 unique values, so after using OneHotEncoding, each of them having a unique column. This will simply blowup the dimensions!\n",
    "- 29% of data entries have at least one missing value. Majority of them fall into \"Materials\" (18%) or \"Manufacturer\" columns (14%).\n",
    "- Price columns contains a mixture of GBP, EUR, CHF, DKK. As seen below, almost 95% of all values have EUR as currency, prices were put to that currency.\n",
    "<img src=\"https://github.com/dissagaliyeva/boat-rentals/blob/master/data/prices.png\"/>\n",
    "\n",
    "\n",
    "After cleaning categorical columns, the number of unique values dropped drastically:\n",
    "\n",
    "| Column | Before (unique values) | After (unique values) |\n",
    "| --- | --- | --- |\n",
    "| Boat Type | 126 | 14 |\n",
    "| Manufacturer | 910 | 742 |\n",
    "| Type | 24 | 4 |\n",
    "| Material | 11 | 6 |\n",
    "| Location | 2995 | 8 |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Selection & Model Evaluation <a class=\"anchor\" id=\"feature\"/>\n",
    "\n",
    "To see how data is related to each other, we typically use Correlation Matrix. However, our case requires something more powerful. Since Correlation Matrix can only work with numeric data, we need a tool that can reveal some insights about the categorical values! Therefore, I'm using a Power Score Matrix instead [1]. It's much more powerful in many ways, so we will stick to it. Now hidden data can no longer hide!\n",
    "\n",
    "<img src=\"https://github.com/dissagaliyeva/boat-rentals/blob/master/data/pps.png\"/>\n",
    "\n",
    "Below we see some interesting insights:\n",
    "- Length & Width are correlated which is not surprising. We will need to drop one of them, I'll be removing the \"Width\" column.\n",
    "- Manufacturer column has big correlations with \"Material\", \"Boat Type\", and \"Location\" columns. We will also remove this column because of two reasons: multi-collinearity and big number of unique values (over 700).\n",
    "- No column seem to have correlation with our goal - \"Number of views last 7 days\".\n",
    "\n",
    "\n",
    "#### Model\n",
    "Since the dataset has quite a lot of outliers and a big number of categorical data, the best model to use would be **RandomForestRegressor**. It performed better in both time- and accuracy evaluations in comparison to Adaboost and XGBoost. Last two models didn't score better than 25%.\n",
    "\n",
    "The best performing model had the following hyper-parameters:\n",
    "- max features: 'sqrt'\n",
    "- min_samples_leaf: 7\n",
    "- n estimators: 200\n",
    "\n",
    "The overall RMSE score was 120 which is not good and not too bad given the fact that there was only 24 hours given to solve the problem!\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final recommendations <a class=\"anchor\" id=\"recommend\"/>\n",
    "\n",
    "1. Fewer questions\n",
    "\n",
    "One of the tasks was to analyze whether currently asked questions during sign-up are too long. Looking at feature importance can reveal the insights:\n",
    "\n",
    "<img src=\"https://github.com/dissagaliyeva/boat-rentals/blob/master/data/features.png\">\n",
    "\n",
    "As seen above, not all the questions were used in the analysis. The two columns that initially had the biggest number of missing values in (Materials=18% and Manufacturer=14%) turned out to be insignificant. Therefore, to increase the signing-up ratio, we would generally need Location, Price, Length & Width, Year Built information.\n",
    "\n",
    "2. More thorough analysis & better model\n",
    "It's quite logical to suggest spending more time analyzing the dataset to see more patterns. Having had only 24 hours to submit a complete analysis implies that it was rushed and some important points might be overlooked. Therefore, to know for a fact which moves to take next, it's strongly recommended to further analyze the dataset. The possible vectors are discussed next in the \"Future Work\" section.\n",
    "\n",
    "3. Look into the boats with lowest views more in details\n",
    "The analysis provided here gives food for thought for the upcoming analysis. It's interesting how the most expensive boat - Mega Yacht and Houseboat got less views than anticipated. It might be helpful to analyze the data entries further.\n",
    "\n",
    "\n",
    "4. Improved UX and UI\n",
    "While cleaning the dataset, there was a lot of inconsistency present. One big improvement would be to have a drop-down to select a specific category so that there is no bad data introduced.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Future work <a class=\"anchor\" id=\"future\"/>\n",
    "\n",
    "To make the analysis better and more accurate, there will be a need to look into the Location distributions to see if there are any patterns. I believe there should be more \"boat-friendly\" locations where having a boat is a must! Just in general, going back and diving into the dataset is always a good idea to find any new patterns.\n",
    "\n",
    "Next, to see which questions would increase the sign-up ratio, it would be great to run several statistical tests such as A/B.\n",
    "\n",
    "Moreover, find the dimensionality techniques and models that are suitted for datasets with a big number of categorical data.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}